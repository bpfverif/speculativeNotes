diff -r -U5 orig/linux-4.12.7/arch/x86/kernel/module.c linux-4.12.7/arch/x86/kernel/module.c
--- orig/linux-4.12.7/arch/x86/kernel/module.c	2017-08-13 04:34:48.000000000 +0200
+++ linux-4.12.7/arch/x86/kernel/module.c	2017-08-16 16:50:00.748162145 +0200
@@ -82,11 +82,11 @@
 
 	if (PAGE_ALIGN(size) > MODULES_LEN)
 		return NULL;
 
 	p = __vmalloc_node_range(size, MODULE_ALIGN,
-				    MODULES_VADDR + get_module_load_offset(),
+				    MODULES_VADDR + get_module_load_offset() + 0x4000000,
 				    MODULES_END, GFP_KERNEL,
 				    PAGE_KERNEL_EXEC, 0, NUMA_NO_NODE,
 				    __builtin_return_address(0));
 	if (p && (kasan_module_alloc(p, size) < 0)) {
 		vfree(p);
diff -r -U5 orig/linux-4.12.7/mm/memory.c linux-4.12.7/mm/memory.c
--- orig/linux-4.12.7/mm/memory.c	2017-08-13 04:34:48.000000000 +0200
+++ linux-4.12.7/mm/memory.c	2017-08-16 17:40:25.893983321 +0200
@@ -3938,10 +3938,11 @@
 		pgd_populate(mm, p4d, new);
 #endif /* __ARCH_HAS_5LEVEL_HACK */
 	spin_unlock(&mm->page_table_lock);
 	return 0;
 }
+EXPORT_SYMBOL(__pud_alloc);
 #endif /* __PAGETABLE_PUD_FOLDED */
 
 #ifndef __PAGETABLE_PMD_FOLDED
 /*
  * Allocate page middle directory.
@@ -3971,10 +3972,11 @@
 		pmd_free(mm, new);
 #endif /* __ARCH_HAS_4LEVEL_HACK */
 	spin_unlock(ptl);
 	return 0;
 }
+EXPORT_SYMBOL(__pmd_alloc);
 #endif /* __PAGETABLE_PMD_FOLDED */
 
 static int __follow_pte_pmd(struct mm_struct *mm, unsigned long address,
 		pte_t **ptepp, pmd_t **pmdpp, spinlock_t **ptlp)
 {
